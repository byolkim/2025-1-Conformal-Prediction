{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a9c189-5649-47f8-8293-937941670f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import t\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffedd822-80b4-4197-ba31-2d4e673b1745",
   "metadata": {},
   "source": [
    "# A primer on predictive inference\n",
    "\n",
    "## Set-up\n",
    "\n",
    "$$\n",
    "(X, Y) \\sim P_X \\times P_{Y \\mid X}\n",
    "$$\n",
    "\n",
    "* $X$: *features* or *predictors*\n",
    "* $Y$: *response*\n",
    "\n",
    "**Goal.** We want to predict $Y$ given $X$.\n",
    "\n",
    "**Examples.**\n",
    "  1. $X =$ father's height, $Y = $ child's height\n",
    "  2. $X =$ image, $Y = $ label\n",
    "  3. ...\n",
    "\n",
    "*Point prediction.* Use available data to train a model $\\hat{\\mu}$ and do $\\hat{\\mu}(X) \\approx Y$.\n",
    "\n",
    "***Problem.*** How can we quantify the prediction error in $\\hat{\\mu}(X)$?\n",
    "\n",
    "## Prediction interval\n",
    "\n",
    "A $100 \\times (1-\\alpha)\\%$ *prediction interval* for $Y$ is an interval $\\hat{C}$ satisfying\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\left\\{ Y \\in \\hat{C}(X) \\right\\} \\geq 1-\\alpha.\n",
    "$$\n",
    "\n",
    "## Classical example: Sir Francis Galton's data on the heights of parents and their children\n",
    "\n",
    "As a warm-up, let’s consider a classic dataset: Sir Francis Galton’s study on the relationship between the heights of parents and their children. This is a well-studied example that is often used to introduce simple linear regression and the concept of prediction intervals.\n",
    "\n",
    "A bit of a historical background: In the late 19th century, Sir Francis Galton, a pioneer in statistics and heredity studies, investigated the relationship between parents' and their children's heights. Through his analysis, Galton observed that while tall parents tended to have tall children, the children's heights were, on average, closer to the population mean than their parents' heights. He termed this phenomenon \"regression toward mediocrity,\" which we now know as \"regression toward the mean.\" This insight led to the development of the statistical method we call \"regression.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679d8778-7722-4ba9-939c-6835aa0964f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv(\"Galton.txt\", sep=\"\\t\")\n",
    "\n",
    "# Extract the relevant columns\n",
    "X = df[\"Father\"].astype(float)\n",
    "Y = df[\"Height\"].astype(float)\n",
    "\n",
    "# Find the total number of observations and report\n",
    "n = X.shape[0] \n",
    "print(\"n = {:d}\".format(n))\n",
    "\n",
    "# Split the data 9:1 for training:testing\n",
    "idx = np.random.permutation(n)\n",
    "ntr = int(np.floor(0.9*n))\n",
    "idx_tr, idx_te = idx[:ntr], idx[ntr:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37b38f4-76f4-4ca2-a17e-652183d075c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a simple linear regression model using the training set\n",
    "Xtr_with_intercept = sm.add_constant(X[idx_tr])\n",
    "model1 = sm.OLS(Y[idx_tr], Xtr_with_intercept).fit()\n",
    "\n",
    "# Extract prediction and prediction intervals\n",
    "Xrange = np.linspace(X.min(), X.max(), 100)\n",
    "Xrange_with_intercept = sm.add_constant(Xrange)\n",
    "Yhat_summary_frame = model1.get_prediction(Xrange_with_intercept).summary_frame(alpha=0.10)\n",
    "Yhat = Yhat_summary_frame[\"mean\"]\n",
    "Yhat_l = Yhat_summary_frame[\"obs_ci_lower\"]\n",
    "Yhat_u = Yhat_summary_frame[\"obs_ci_upper\"]\n",
    "\n",
    "# Plot the data and the fitted model & the prediction band\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[idx_tr], Y[idx_tr], alpha=0.5, label=\"Training Data\")\n",
    "plt.scatter(X[idx_te], Y[idx_te], alpha=0.5, label=\"Test Data\")\n",
    "plt.plot(Xrange, Yhat, color=\"red\", label=\"Fitted Model\")\n",
    "plt.fill_between(Xrange, Yhat_l, Yhat_u, color=\"gray\", alpha=0.3, label=\"90% Prediction Band\")\n",
    "plt.xlabel(\"Father's Height\")\n",
    "plt.ylabel(\"Child's Height\")\n",
    "plt.title(\"Regression of Child's Height on Father's Height\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Compute the coverage on the test data and report\n",
    "Xte_with_intercept = sm.add_constant(X[idx_te])\n",
    "Ytehat_summary_frame = model1.get_prediction(Xte_with_intercept).summary_frame(alpha=0.10)\n",
    "Ytehat = Ytehat_summary_frame[\"mean\"]\n",
    "Ytehat_l = Ytehat_summary_frame[\"obs_ci_lower\"]\n",
    "Ytehat_u = Ytehat_summary_frame[\"obs_ci_upper\"]\n",
    "coverage1 = np.mean(np.logical_and(Ytehat_l <= Y[idx_te], Y[idx_te] <= Ytehat_u))\n",
    "print(\"Test coverage is {:f}\".format(coverage1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e24f9c7-0a4f-46d1-b28e-e142169b1060",
   "metadata": {},
   "source": [
    "The above prediction band can be computed as\n",
    "\n",
    "$$\n",
    "\\hat{C}(x) = \\hat{\\beta}_0+\\hat{\\beta}_1 x \\pm t_{n-2, 1-\\alpha/2} \\hat{\\sigma} \\sqrt{1+\\frac{1}{n}+\\frac{(x-\\bar{X})^2}{\\sum_{i=1}^{n} (X_i-\\bar{X})^2}}\n",
    "$$\n",
    "\n",
    "**Question.** What are the assumptions under which this prediction interval is valid?\n",
    "\n",
    "## Synthetic data example\n",
    "\n",
    "Now, let's look at a synthetic dataset due to Romano et al. (2019) that obviously violates the assumptions of simple linear regression. Among many things, this dataset is characterized by heteroscedasticity, meaning the variability of the response variable changes across different levels of the predictor. In such cases, the prediction intervals derived from simple linear models become unreliable in the sense that a 90% prediction interval may undercover or overcover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9d3c6d-024a-49ce-bd3c-a7e700e5b6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    ''' Construct data (1D example)\n",
    "    '''\n",
    "    ax = 0*x\n",
    "    for i in range(len(x)):\n",
    "        ax[i] = (np.random.poisson(np.sin(x[i])**2+0.1) + 0.03*x[i]*np.random.randn(1))[0]\n",
    "        ax[i] += (25*(np.random.uniform(0,1,1)<0.01)*np.random.randn(1))[0]\n",
    "    return ax.astype(np.float32)\n",
    "\n",
    "# Generate training data\n",
    "Xtr = np.random.uniform(0, 5.0, size=ntr).astype(np.float32)\n",
    "Ytr = f(Xtr)\n",
    "\n",
    "# Generate test data\n",
    "Xte = np.random.uniform(0, 5.0, size=n-ntr).astype(np.float32)\n",
    "Yte = f(Xte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d547e39d-fbf6-46cf-86a3-33fe432092fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(Xtr, Ytr, alpha=0.5, label=\"Training Data\")\n",
    "plt.scatter(Xte, Yte, alpha=0.5, label=\"Test Data\")\n",
    "plt.xlim([0, 5.0])\n",
    "plt.xlabel(\"$X$\")\n",
    "plt.ylabel(\"$Y$\")\n",
    "plt.title(\"Romano et al. (2019)'s Synthetic Data\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the data without outliers (zoom in)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(Xtr, Ytr, alpha=0.5, label=\"Training Data\")\n",
    "plt.scatter(Xte, Yte, alpha=0.5, label=\"Test Data\")\n",
    "plt.xlim([0, 5.0])\n",
    "plt.ylim([-2.5, 7])\n",
    "plt.xlabel(\"$X$\")\n",
    "plt.ylabel(\"$Y$\")\n",
    "plt.title(\"Romano et al. (2019)'s Synthetic Data (Zoomed in)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54f189b-528a-4a51-93f4-0122b13dbfe1",
   "metadata": {},
   "source": [
    "Let's pretend we don't know what we are doing, and fit a simple linear model anyway and compute the prediction band as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a414669-4070-481b-b9b5-572b0ff0cea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a simple linear regression model using the training set\n",
    "Xtr_with_intercept = sm.add_constant(Xtr)\n",
    "model2 = sm.OLS(Ytr, Xtr_with_intercept).fit()\n",
    "\n",
    "# Extract prediction and prediction intervals\n",
    "Xrange = np.linspace(0, 5.0, 100)\n",
    "Xrange_with_intercept = sm.add_constant(Xrange)\n",
    "Yhat_summary_frame = model2.get_prediction(Xrange_with_intercept).summary_frame(alpha=0.10)\n",
    "Yhat = Yhat_summary_frame[\"mean\"]\n",
    "Yhat_l = Yhat_summary_frame[\"obs_ci_lower\"]\n",
    "Yhat_u = Yhat_summary_frame[\"obs_ci_upper\"]\n",
    "\n",
    "# Plot the data and the fitted model & the prediction band\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(Xtr, Ytr, alpha=0.5, label=\"Training Data\")\n",
    "plt.scatter(Xte, Yte, alpha=0.5, label=\"Test Data\")\n",
    "plt.plot(Xrange, Yhat, color=\"red\", label=\"Fitted Model\")\n",
    "plt.fill_between(Xrange, Yhat_l, Yhat_u, color=\"gray\", alpha=0.3, label=\"90% Prediction Band\")\n",
    "plt.xlim([0, 5.0])\n",
    "plt.xlabel(\"$X$\")\n",
    "plt.ylabel(\"$Y$\")\n",
    "plt.title(\"OLS Homoscedastic Gaussian Error Prediction Band\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Compute the coverage on the test data and report\n",
    "Xte_with_intercept = sm.add_constant(Xte)\n",
    "Ytehat_summary_frame = model2.get_prediction(Xte_with_intercept).summary_frame(alpha=0.10)\n",
    "Ytehat = Ytehat_summary_frame[\"mean\"]\n",
    "Ytehat_l = Ytehat_summary_frame[\"obs_ci_lower\"]\n",
    "Ytehat_u = Ytehat_summary_frame[\"obs_ci_upper\"]\n",
    "coverage2 = np.mean(np.logical_and(Ytehat_l <= Yte, Yte <= Ytehat_u))\n",
    "print(\"Test coverage is {:f}\".format(coverage2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a946573-0f52-431e-8b15-ba79f7242895",
   "metadata": {},
   "source": [
    "*If* the simple linear regression prediction interval had the correct coverage, then it would cover the true test observation about 90% of the time. However, we see that the number is nowhere close.\n",
    "\n",
    "As mentioned above, the biggest problem with this data is heteroscedasticity, i.e., non-constant variability around the conditional mean. Of course, we can try to do a better job by also trying to model the conditional variance. (In fact, this is precisely what we will do when we revisit this example for the conformalized quantile regression (CQR) later.) However, given additional i.i.d. data, there is an intuitive way to construct a prediction interval that will always have 90% coverage with the same model.\n",
    "\n",
    "Now, suppose we have a *calibration* dataset which consist of additional $m$ i.i.d. observations generated from the same distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922bd14c-e423-4d68-a919-e62cf1057a69",
   "metadata": {},
   "source": [
    "Now, let's do a bit of a thought experiment. Suppose I have $m+1$ i.i.d. random variables\n",
    "$$\n",
    "S_1, \\dots, S_m, S'\n",
    "$$\n",
    "\n",
    "1. What is the probability that $S'$ is the smallest?\n",
    "2. What is the probability that $S'$ is the second smallest?\n",
    "3. What is the probability that $S'$ is the $k$-th smallest for any $k$?\n",
    "\n",
    "Let's change the question.\n",
    "1. What is the probability that $S'$ is less than or equal to the smallest of $S_1, \\dots, S_m$?\n",
    "2. What is the probability that $S'$ is less than or equal to the second smallest of $S_1, \\dots, S_m$?\n",
    "3. What is the probability that $S'$ is less than or equal to the $k$-th smallest of $S_1, \\dots, S_m$?\n",
    "\n",
    "In general, $$\\mathbb{P}\\left\\{S' \\leq S_{(k)}\\right\\} = \\frac{k}{m+1}.$$ This is *always* true without *any* assumptions on the distribution of $S$ as long as the data points are i.i.d. (and more generally, we shall see, exchangeable).\n",
    "\n",
    "Now, let's return to the synthetic data example. Because the simple linear model $\\hat{\\mu}$ was fitted using the training data, which are independent of both the test and the calibration data, we have that $$S_1 = |Y_1-\\hat{\\mu}(X_1)|, \\dots, S_m = |Y_m-\\hat{\\mu}(X_m)|, S' = |Y'-\\hat{\\mu}(X')|$$ are i.i.d., where $(X_1,Y_1), \\dots, (X_m,Y_m)$ are the calibration data and $(X',Y')$ is any point in the test data.\n",
    "\n",
    "Thus, defining $$\\hat{q}_{1-\\alpha} = \\text{the $\\lceil (1-\\alpha) (m+1) \\rceil$-th smallest of $S_1, \\dots, S_m$,}$$ we have $$\\mathbb{P}\\left\\{|Y'-\\hat{\\mu}(X')| \\leq \\hat{q}_{1-\\alpha}\\right\\} = \\frac{\\lceil (1-\\alpha) (m+1) \\rceil}{m+1} \\geq \\frac{(1-\\alpha)(m+1)}{m+1} = 1-\\alpha.$$ However, $$\\left\\{|Y'-\\hat{\\mu}(X')| \\leq \\hat{q}_{1-\\alpha}\\right\\} = \\left\\{Y' \\in \\hat\\mu(X') \\pm \\hat{q}_{1-\\alpha} \\right\\}.$$ Therefore, the interval $$\\tilde{C}(x) = \\hat{\\mu}(x) \\pm \\hat{q}_{1-\\alpha}$$ *always* satisfies $$\\mathbb{P}\\left\\{Y' \\in \\tilde{C}(X')\\right\\} \\geq 1-\\alpha.$$ Note that we did not have to make any assumptions on the accuracy of $\\hat{\\mu}$ or the data distribution.\n",
    "\n",
    "You will get to look at this argument in more detail when we go over the basic theory of conformal prediction in a couple of weeks. For now, let's just check the idea actually works.\n",
    "\n",
    "We start by generating calibration data. To keep things simple, I'm reusing $m = n_{\\text{tr}}$. However, feel free to experiment with my code and see for yourself that the coverage guarantee has nothing to do with $m$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27843c0-de50-49c0-95d2-f3b11c46b6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = ntr\n",
    "\n",
    "# Generate calibration data\n",
    "Xcal = np.random.uniform(0, 5.0, size=m).astype(np.float32)\n",
    "Ycal = f(Xcal)\n",
    "\n",
    "# Compute the nonconformity scores, sort, and get the (1-alpha)(m+1)-th smallest element\n",
    "S = np.abs(Ycal-model2.predict(sm.add_constant(Xcal)))\n",
    "S = np.sort(S)\n",
    "qhat = S[int(np.ceil((1-0.1)*(m+1)))-1] # subtract 1, because Python indices start from 0 not 1\n",
    "\n",
    "# Plot the new prediction band\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(Xtr, Ytr, alpha=0.5, label=\"Training Data\")\n",
    "plt.scatter(Xte, Yte, alpha=0.5, label=\"Test Data\")\n",
    "plt.plot(Xrange, Yhat, color=\"red\", label=\"Fitted Model\")\n",
    "plt.fill_between(Xrange, Yhat-qhat, Yhat+qhat, color=\"gray\", alpha=0.3, label=\"90% Prediction Band\")\n",
    "plt.xlim([0, 5.0])\n",
    "plt.xlabel(\"$X$\")\n",
    "plt.ylabel(\"$Y$\")\n",
    "plt.title(\"OLS Conformal Prediction Band\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Compute the coverage on the test data and report\n",
    "coverage3 = np.mean(np.abs(Yte-Ytehat) <= qhat)\n",
    "print(\"Test coverage is {:f}\".format(coverage3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19baf9bd-3789-4b96-aa4c-21132b34dd0a",
   "metadata": {},
   "source": [
    "For these linear regression examples with continuous responses, it was somewhat natural to start by measuring the conformity (or lack thereof) as $$s(x, y) = |y-\\hat{\\mu}(x)|.$$ This is an example of *nonconformity score* (alternatively, just *conformity score*). For different types of problems, other nonconformity scores will be more natural, e.g., image classification problems.\n",
    "\n",
    "Roughly speaking, a nonconformity score measures how closely an observation sticks to a given model, in this case $\\hat{\\mu}$ from the simple linear regression. The more an observation \"conforms\" to the given model the smaller its nonconformity score, and *vice versa*, and hence, the term *conformal prediction*.\n",
    "\n",
    "Next time, in the first main tutorial, we will look at an image classification example and how the conformal prediction can be used to provide valid uncertainty quantification for even such unstructured problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
