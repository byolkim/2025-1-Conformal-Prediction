{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial has been modified from imagenet-smallest-sets.ipynb from conformal-prediction (https://github.com/aangelopoulos/conformal-prediction). It is based on Sadinle et al. (2016) (https://arxiv.org/abs/1609.00451). This worksheet in particular was created with the help of Claude 3.7.\n",
    "\n",
    "# Worksheet: Image classification example using ImageNet\n",
    "\n",
    "In this tutorial, we will apply conformal prediction to an image classification problem to obtain prediction sets of plausible labels using a pre-trained model. This means we will make use of the model outputs rather than training an image classifier from scratch.\n",
    "\n",
    "## What is conformal prediction?\n",
    "\n",
    "Conformal prediction is a framework that allows us to quantify uncertainty in machine learning predictions by producing prediction sets that are guaranteed to contain the true label with a specified probability (e.g., 90%). Unlike traditional machine learning methods that output a single prediction, conformal prediction produces a set of plausible predictions with a statistical guarantee on its coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "!pip install -U --no-cache-dir gdown --pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires 1.31G space!!\n",
    "# Download the data. The data include softmax scores from a pre-trained ResNet-152 model on ImageNet\n",
    "if not os.path.exists('../data'):\n",
    "    os.system('gdown 1h7S6N_Rx7gdfO3ZunzErZy6H7620EbZK -O ../data.tar.gz')\n",
    "    os.system('tar -xf ../data.tar.gz -C ../')\n",
    "    os.system('rm ../data.tar.gz')\n",
    "if not os.path.exists('../data/imagenet/human_readable_labels.json'):\n",
    "    !wget -nv -O ../data/imagenet/human_readable_labels.json -L https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\n",
    "\n",
    "# Load the data\n",
    "data = np.load('../data/imagenet/imagenet-resnet152.npz') # softmax scores-label pairs\n",
    "example_paths = os.listdir('../data/imagenet/examples') # path to actual image files\n",
    "smx = data['smx'] # softmax scores of images from a pre-trained model\n",
    "labels = data['labels'].astype(int) # true labels\n",
    "\n",
    "# Examine the data shape\n",
    "print(f\"Shape of smx: {smx.shape}\") # shows the number of images and number of classes\n",
    "print(f\"Number of example images: {len(example_paths)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row of $\\texttt{smx}$ is the *softmax scores* of an image, which we can think of as estimated probabilities that the given image belongs to each of $K = 1000$ possible classes. Symbolically, $$\\texttt{smx}[i,] = \\hat{f}(\\text{Image}_i) \\in [0,1]^{K}, \\quad \\texttt{smx}[i, j] \\approx \\mathbb{P}\\{\\text{Image}_i \\text{ has Label } j\\} \\text{ (according to $\\hat{f}$)}.$$\n",
    "\n",
    "For this example, we are going to use the nonconformity score $$s(x, y) = 1-\\hat{f}(x)_y,$$ i.e., 1-the softmax score for the *true* class, which has the interpretation of the probability of the image $x$ *not* belonging to the true class *according* to the model $\\hat{f}$.\n",
    "\n",
    "**Think.** Why does this nonconformity score make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: Set the parameters\n",
    "# alpha controls the error rate we're willing to accept\n",
    "# alpha = 0.1 means we want our prediction sets to contain the true label at least 90% of the time\n",
    "alpha = # TODO: Set alpha to 0.1 for 90% coverage\n",
    "m = # TODO: Set m to 1000 (number of calibration points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: Split the data into calibration and test sets\n",
    "# Create a Boolean mask for selecting calibration points\n",
    "idx = # TODO: Create a Boolean array with m True values and the rest False\n",
    "np.random.shuffle(idx) # shuffle to randomly select calibration points\n",
    "\n",
    "# Split the data using the mask\n",
    "smx_cal, smx_te = # TODO: Split smx into calibration and test sets using idx\n",
    "labels_cal, labels_te = # TODO: Split labels into calibration and test sets using idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: Compute the nonconformity scores for the calibration set\n",
    "# Hint: For each calibration point, the score is 1 minus the softmax score for the true class\n",
    "S_cal = # TODO:\n",
    "\n",
    "# Sort the scores\n",
    "S_cal = np.sort(S_cal)\n",
    "\n",
    "# EXERCISE: Find the threshold (quantile) that ensures the desired coverage\n",
    "qhat = # TODO: Find the appropriate quantile of S_cal to ensure 1-alpha coverage\n",
    "\n",
    "print(f\"Threshold (qhat): {qhat:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the distribution of nonconformity scores and our chosen threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of nonconformity scores\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(S_cal, bins=30, alpha=0.7)\n",
    "plt.axvline(qhat, color='red', linestyle='--', label=f'Threshold (qhat = {qhat:.3f})')\n",
    "plt.xlabel('Nonconformity Score (1 - softmax score of true class)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Nonconformity Scores in Calibration Set')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, applying the same idea as discussed in the previous class, we have that for a test image $X'$ with *unknown* label $Y'$, $$\\mathbb{P}\\left\\{S' = 1-\\hat{f}(X')_{Y'} \\leq \\hat{q}_{1-\\alpha} \\right\\} \\geq 1-\\alpha.$$ Therefore, the set $$\\hat{C}(x) = \\left\\{y: \\hat{f}(x)_y \\geq 1-\\hat{q}_{1-\\alpha}\\right\\}$$ must satisfy $$\\mathbb{P}\\left\\{Y' \\in \\hat{C}(X')\\right\\} \\geq 1-\\alpha.$$ This is how we construct prediction sets in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: Construct the prediction sets for the test data\n",
    "Chat = # TODO: Create a Boolean matrix where entry (i,j) is True if class j is in the prediction set for test point i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: Check the empirical coverage\n",
    "# Hint: The true label is covered if the corresponding entry in Chat is True\n",
    "empirical_coverage = # TODO:\n",
    "print(f\"The empirical coverage is: {empirical_coverage:.4f}\")\n",
    "print(f\"The target coverage was: {1-alpha:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the sizes of prediction sets\n",
    "set_sizes = Chat.sum(axis=1)\n",
    "avg_size = set_sizes.mean()\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(set_sizes, bins=30, alpha=0.7)\n",
    "plt.axvline(avg_size, color='red', linestyle='--', label=f'Average size: {avg_size:.2f}')\n",
    "plt.xlabel('Prediction Set Size')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Prediction Set Sizes')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(f\"Average prediction set size: {avg_size:.2f} out of 1000 possible classes\")\n",
    "print(f\"Min set size: {set_sizes.min()}, Max set size: {set_sizes.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a look at some examples and see what conformal prediction gave us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the human-readable labels\n",
    "with open('../data/imagenet/human_readable_labels.json') as f:\n",
    "    label_strings = np.array(json.load(f))\n",
    "\n",
    "# EXERCISE: Complete the function to display an example image and its prediction set\n",
    "def display_example(img_index):\n",
    "    # Load the image\n",
    "    img_path = f'../data/imagenet/examples/{img_index}.JPEG' \n",
    "    img = imread(img_path)\n",
    "    \n",
    "    # Get the true label and prediction set\n",
    "    true_label = # TODO: Get the true label for this image\n",
    "    prediction_set = # TODO: Determine which classes are in the prediction set\n",
    "    \n",
    "    # Display the image\n",
    "    plt.figure()\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print information\n",
    "    print(\"The true label is:\", str(label_strings[true_label]))\n",
    "    print(f\"Prediction set size: {np.sum(prediction_set)} classes\")\n",
    "    print(\"The prediction set includes: %s\" % \", \".join(map(str, list(label_strings[prediction_set]))))\n",
    "    \n",
    "    # Return whether the true label is in the prediction set\n",
    "    return prediction_set[true_label]\n",
    "\n",
    "# Display a few random examples\n",
    "example_paths = os.listdir('../data/imagenet/examples')\n",
    "for i in range(5):\n",
    "    rand_path = np.random.choice(example_paths)\n",
    "    img_index = int(rand_path.split('.')[0])\n",
    "    covered = display_example(img_index)\n",
    "    print(f\"True label covered: {covered}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE: Experiment with different alpha values\n",
    "\n",
    "Try changing alpha to different values (e.g., 0.01, 0.05, 0.2) and observe how it affects:\n",
    "1. The threshold (qhat)\n",
    "2. The empirical coverage\n",
    "3. The sizes of the prediction sets\n",
    "\n",
    "Complete the function below to rerun the conformal prediction with a new alpha value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_conformal_prediction(alpha, m=1000):\n",
    "    # Split the data\n",
    "    idx = np.array([1] * m + [0] * (smx.shape[0]-m)) > 0\n",
    "    np.random.shuffle(idx)\n",
    "    smx_cal, smx_te = smx[idx,:], smx[~idx,:]\n",
    "    labels_cal, labels_te = labels[idx], labels[~idx]\n",
    "    \n",
    "    # Compute nonconformity scores\n",
    "    S_cal = 1 - smx_cal[np.arange(m), labels_cal]\n",
    "    S_cal = np.sort(S_cal)\n",
    "    \n",
    "    # Find the threshold\n",
    "    qhat = # TODO: Calculate the threshold\n",
    "    \n",
    "    # Construct prediction sets\n",
    "    Chat = # TODO: Construct prediction sets\n",
    "    \n",
    "    # Check empirical coverage\n",
    "    empirical_coverage = # TODO: Calculate the empirical coverage\n",
    "    \n",
    "    # Calculate set sizes\n",
    "    set_sizes = Chat.sum(axis=1)\n",
    "    \n",
    "    return {\n",
    "        'alpha': alpha,\n",
    "        'target_coverage': 1-alpha,\n",
    "        'qhat': qhat,\n",
    "        'empirical_coverage': empirical_coverage,\n",
    "        'avg_set_size': set_sizes.mean(),\n",
    "        'min_set_size': set_sizes.min(),\n",
    "        'max_set_size': set_sizes.max()\n",
    "    }\n",
    "\n",
    "# Try different alpha values\n",
    "results = []\n",
    "for alpha in [0.01, 0.05, 0.1, 0.2]:\n",
    "    result = run_conformal_prediction(alpha)\n",
    "    results.append(result)\n",
    "    print(f\"Alpha: {alpha}, Target coverage: {1-alpha:.2f}\")\n",
    "    print(f\"Empirical coverage: {result['empirical_coverage']:.4f}\")\n",
    "    print(f\"Average set size: {result['avg_set_size']:.2f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Plot the relationship between coverage and set size\n",
    "plt.figure(figsize=(10, 5))\n",
    "coverages = [r['target_coverage'] for r in results]\n",
    "set_sizes = [r['avg_set_size'] for r in results]\n",
    "plt.plot(coverages, set_sizes, 'o-')\n",
    "plt.xlabel('Target Coverage (1-alpha)')\n",
    "plt.ylabel('Average Prediction Set Size')\n",
    "plt.title('Trade-off between Coverage and Prediction Set Size')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we demonstrated how to apply conformal prediction to image classification:\n",
    "\n",
    "1. **Problem Setup**: We worked with pre-trained ResNet152 model outputs on ImageNet, which gave us softmax scores for 1000 classes.\n",
    "\n",
    "2. **Nonconformity Score**: We used a simple nonconformity score: 1 minus the softmax score for the true class, representing the model's estimated probability that the image does not belong to its true class.\n",
    "\n",
    "3. **Calibration Process**: We:\n",
    "   - Split the data into calibration and test sets\n",
    "   - Computed nonconformity scores for the calibration set\n",
    "   - Found the threshold (quantile) that ensures the desired coverage level\n",
    "\n",
    "4. **Prediction Sets**: For each test image, we included all classes with softmax scores high enough to meet our threshold criterion.\n",
    "\n",
    "5. **Coverage Guarantee**: We verified that our prediction sets achieved the desired coverage rate empirically.\n",
    "\n",
    "6. **LABEL Method**: This approach (Least-Ambiguous with Bounded Error Levels) is simple but effective, producing prediction sets that are guaranteed to contain the true label with high probability.\n",
    "\n",
    "7. **Limitations**: The method doesn't adapt to the difficulty of individual examples, which more advanced conformal prediction methods can address.\n",
    "\n",
    "The key benefit of conformal prediction is that it provides statistically valid uncertainty quantification, regardless of the underlying model. This allows us to make reliable predictions with guaranteed error rates, which is essential in high-stakes applications.\n",
    "\n",
    "## Discussion Questions\n",
    "\n",
    "1. How does the size of the prediction sets relate to the model's confidence in its predictions? How would you expect the prediction set sizes to change if we used a less accurate model?\n",
    "\n",
    "2. What happens to the prediction sets as we change the coverage level (1-alpha)? What are the trade-offs involved in choosing different values of alpha?\n",
    "\n",
    "3. In what real-world applications would having prediction sets (rather than single predictions) be particularly valuable? When might it be less useful?\n",
    "\n",
    "4. The nonconformity score we used (1 - softmax score for the true class) doesn't adapt to the difficulty of each example. Can you think of alternative nonconformity scores that might better account for example difficulty?\n",
    "\n",
    "5. How does the calibration set size affect the properties of our prediction sets? What would happen if we used a very small or very large calibration set?\n",
    "\n",
    "6. Conformal prediction is model-agnostic. How would the process change if we wanted to use a different base model (e.g., ViT instead of ResNet)?\n",
    "\n",
    "7. How does conformal prediction compare to other uncertainty quantification methods you might be familiar with (e.g., Bayesian methods, ensemble methods)?\n",
    "\n",
    "8. Can you think of ways to visualize or interpret the prediction sets to gain insights about the model's behavior and limitations?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
